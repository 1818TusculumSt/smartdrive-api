import os
import sys
import json
import logging
import re
from typing import List, Dict, Tuple
from mcp.server import Server
from mcp.types import Tool, TextContent
from pinecone import Pinecone
from embeddings import EmbeddingProvider
from config import settings
from document_storage import DocumentStorage
from sentence_transformers import CrossEncoder

# Configure logging to stderr ONLY (stdout is reserved for MCP protocol)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stderr  # CRITICAL: MCP uses stdout for JSON-RPC, logs must go to stderr
)

# Silence noisy Azure SDK logging
logging.getLogger('azure').setLevel(logging.WARNING)
logging.getLogger('azure.core.pipeline.policies.http_logging_policy').setLevel(logging.WARNING)

# Initialize
app = Server("smartdrive-mcp")
pc = Pinecone(api_key=settings.PINECONE_API_KEY)
index = pc.Index(
    name=settings.PINECONE_INDEX_NAME,
    host=settings.PINECONE_HOST
)
embedding_provider = EmbeddingProvider()
document_storage = DocumentStorage()

# Initialize reranker model (lightweight, fast)
logging.info("Loading reranking model...")
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
logging.info("Reranker loaded successfully")

# ============================================================================
# INTELLIGENT SEARCH ENHANCEMENTS
# ============================================================================

def preprocess_query(query: str) -> str:
    """
    Enhance queries with semantic expansion and cleanup.
    Mimics what Claude does naturally through conversation.
    """
    query_lower = query.lower()

    # Tax-related queries
    tax_keywords = ["tax", "1099", "w-2", "w2", "w-9", "1040", "irs", "fiscal"]
    if any(kw in query_lower for kw in tax_keywords):
        query = f"{query} tax document IRS form fiscal year"

    # Financial documents
    financial_keywords = ["invoice", "receipt", "bill", "payment", "budget", "expense"]
    if any(kw in query_lower for kw in financial_keywords):
        query = f"{query} financial accounting expense"

    # Meeting/notes
    meeting_keywords = ["meeting", "notes", "minutes", "agenda"]
    if any(kw in query_lower for kw in meeting_keywords):
        query = f"{query} meeting notes discussion agenda"

    # Project/proposal
    project_keywords = ["project", "proposal", "plan", "roadmap"]
    if any(kw in query_lower for kw in project_keywords):
        query = f"{query} project plan proposal strategy"

    # Reports
    report_keywords = ["report", "analysis", "summary", "quarterly"]
    if any(kw in query_lower for kw in report_keywords):
        query = f"{query} report analysis summary data"

    # Remove common filler words that don't help semantic search
    filler_words = ["find", "search", "show me", "give me", "get me", "look for"]
    for filler in filler_words:
        query = re.sub(r'\b' + re.escape(filler) + r'\b', '', query, flags=re.IGNORECASE)

    # Clean up extra whitespace
    query = ' '.join(query.split())

    return query


def generate_query_variations(query: str) -> List[str]:
    """
    Generate multiple query variations to cast a wider semantic net.
    Mimics Claude's multi-attempt search strategy.
    """
    variations = [query]  # Always include original

    query_lower = query.lower()

    # Add variations without filler words
    cleaned = re.sub(r'\b(find|search|show|give|get|look)\s+(me|for)?\s*', '', query, flags=re.IGNORECASE).strip()
    if cleaned and cleaned != query:
        variations.append(cleaned)

    # Add keyword-focused version (just the nouns/important terms)
    keywords = re.findall(r'\b[A-Za-z0-9]+\b', query)
    # Filter out common words
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'find', 'search', 'show', 'me', 'my'}
    important_keywords = [kw for kw in keywords if kw.lower() not in stopwords]
    if len(important_keywords) >= 2:
        variations.append(' '.join(important_keywords))

    # Deduplicate while preserving order
    seen = set()
    unique_variations = []
    for v in variations:
        v_clean = v.strip()
        if v_clean and v_clean not in seen:
            seen.add(v_clean)
            unique_variations.append(v_clean)

    logging.info(f"Generated {len(unique_variations)} query variations: {unique_variations}")
    return unique_variations


def rerank_results(query: str, doc_results: Dict[str, Dict]) -> List[Tuple[str, Dict]]:
    """
    Rerank search results using a cross-encoder model for better relevance.
    This is more accurate than vector similarity alone.
    """
    if not doc_results:
        return []

    # Prepare pairs for reranking: [query, document_text]
    doc_ids = list(doc_results.keys())
    pairs = []
    for doc_id in doc_ids:
        doc_info = doc_results[doc_id]
        # Use first 1000 chars for reranking (balance between context and speed)
        text_snippet = doc_info['full_text'][:1000]
        pairs.append([query, text_snippet])

    # Get reranking scores
    logging.info(f"Reranking {len(pairs)} documents...")
    rerank_scores = reranker.predict(pairs)

    # Combine original scores with rerank scores
    reranked = []
    for doc_id, rerank_score in zip(doc_ids, rerank_scores):
        doc_info = doc_results[doc_id]
        # Blend original Pinecone score (0-1) with rerank score (-10 to 10)
        # Normalize rerank score to 0-1 range and blend 50/50
        normalized_rerank = (rerank_score + 10) / 20  # Map [-10, 10] to [0, 1]
        blended_score = (doc_info['score'] + normalized_rerank) / 2

        doc_info['original_score'] = doc_info['score']
        doc_info['rerank_score'] = float(rerank_score)
        doc_info['blended_score'] = float(blended_score)

        reranked.append((doc_id, doc_info))

    # Sort by blended score (descending)
    reranked.sort(key=lambda x: x[1]['blended_score'], reverse=True)

    logging.info(f"Reranking complete. Top result score: {reranked[0][1]['blended_score']:.3f}")
    return reranked

async def _perform_search(original_query: str, top_k: int = 5) -> List[Tuple[str, Dict]]:
    """
    Internal helper to perform the full search pipeline:
    1. Preprocess query
    2. Generate variations
    3. Hybrid search (dense + sparse) for all variations
    4. Rerank results
    """
    logging.info(f"ğŸ” Search request: '{original_query}' (top_k={top_k})")

    # ENHANCEMENT 1: Preprocess query (add semantic context)
    enhanced_query = preprocess_query(original_query)
    logging.info(f"ğŸ“ Enhanced query: '{enhanced_query}'")

    # ENHANCEMENT 2: Generate query variations
    query_variations = generate_query_variations(enhanced_query)

    # Collect results from all query variations
    all_doc_results = {}
    queries_attempted = 0

    # Search with enhanced query + variations (max 3 variations to avoid slowdown)
    for query_variant in query_variations[:3]:
        queries_attempted += 1
        logging.info(f"ğŸ” Searching with variant {queries_attempted}: '{query_variant}'")

        # Generate dense query embedding (semantic)
        query_embedding = await embedding_provider.get_embedding(query_variant)

        if query_embedding is None:
            logging.warning(f"âš ï¸ Failed to generate embedding for variant: '{query_variant}'")
            continue

        query_embedding = query_embedding.tolist()

        # Generate sparse query embedding (keyword/BM25) for hybrid search
        sparse_query_embedding = await embedding_provider.get_sparse_embedding(query_variant)

        # Search Pinecone with hybrid search (dense + sparse)
        # Fetch more results than requested for reranking (top_k * 4)
        fetch_count = min(top_k * 4, 20)  # Max 20 for reranking
        query_params = {
            "vector": query_embedding,
            "top_k": fetch_count,
            "namespace": "smartdrive",
            "include_metadata": True
        }

        # Add sparse vector if generated successfully and contains values
        # Pinecone requires sparse vectors to have at least one value
        if sparse_query_embedding and sparse_query_embedding.get("values") and len(sparse_query_embedding["values"]) > 0:
            query_params["sparse_vector"] = sparse_query_embedding
        else:
            logging.info(f"No sparse values generated for query variant '{query_variant}', using dense-only search")

        results = index.query(**query_params)

        # Collect documents from this query variant
        for match in results.matches:
            meta = match.metadata
            doc_id = meta.get('doc_id')

            # DEBUG: Log what we're getting from Pinecone
            logging.info(f"  ğŸ“‹ Pinecone match - doc_id: '{doc_id}', file: '{meta.get('file_name', 'N/A')}', score: {match.score:.3f}")

            if doc_id and doc_id not in all_doc_results:
                # First time seeing this document - retrieve full text from Azure Blob
                full_text = document_storage.retrieve_document(doc_id)

                if full_text:
                    all_doc_results[doc_id] = {
                        "file_name": meta.get('file_name', 'Unknown'),
                        "file_path": meta.get('file_path', 'Unknown'),
                        "modified": meta.get('modified', 'Unknown'),
                        "score": match.score,
                        "full_text": full_text,
                        "source_query": query_variant
                    }

    # Check if we found anything
    if not all_doc_results:
        return []

    logging.info(f"ğŸ“Š Found {len(all_doc_results)} unique documents across {queries_attempted} query variations")

    # ENHANCEMENT 3: Rerank results using CrossEncoder
    reranked_results = rerank_results(original_query, all_doc_results)

    # Take only top_k after reranking
    return reranked_results[:top_k]

@app.list_tools()
async def list_tools() -> list[Tool]:
    """List available tools"""
    return [
        Tool(
            name="search_onedrive",
            description="""
            ğŸ” INTELLIGENT HYBRID SEARCH across OneDrive documents.

            COMBINES:
            - Semantic search (understands meaning, not just keywords)
            - Keyword/BM25 search (exact term matching)
            - Multi-query expansion (searches multiple variations)
            - AI reranking (reorders results for maximum relevance)

            HOW TO USE:
            - Use natural language queries (e.g., "tax documents from 2024")
            - Combine specific terms with context (e.g., "project proposal Q4 budget")
            - Include file types if relevant (e.g., "Excel budget spreadsheet")
            - Time periods work great (e.g., "meeting notes March 2024")

            EXAMPLE QUERIES THAT WORK GREAT:
            âœ“ "1099 tax forms" â†’ finds W-2s, 1099s, tax documents, IRS forms
            âœ“ "meeting notes about the budget" â†’ finds meeting notes discussing budgets
            âœ“ "quarterly report Q4 2024" â†’ finds Q4 2024 reports
            âœ“ "project proposal client presentation" â†’ finds proposals and presentations
            âœ“ "invoice from Acme Corp" â†’ finds invoices from that company

            RETURNS:
            Top matching documents with:
            - File names, paths, and modification dates
            - Relevance scores (higher = better match)
            - Content previews (first 2000 chars)
            - Document IDs (use with read_document for full text)

            TIPS FOR BEST RESULTS:
            - Be specific: "2024 tax forms" > "taxes"
            - Include context: "Q4 budget meeting notes" > "notes"
            - Use multiple keywords: "invoice payment receipt" > "invoice"
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search query - use natural language, be specific, include context"
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "Number of results to return (default: 5, max: 20)",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 20
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="read_document",
            description="""
            ğŸ“„ RETRIEVE FULL DOCUMENT TEXT from Azure Blob Storage.

            Use this when you need the complete content of a document found in search results.
            Every search result includes a Document ID - pass that ID here to get the full text.

            WHEN TO USE:
            - After finding a document with search_onedrive
            - When you need more than the 2000-char preview
            - To analyze complete document contents
            - To extract specific information from the full text

            EXAMPLE:
            1. search_onedrive("2024 tax forms")
               â†’ Returns: doc_abc123 (W-2 form, 15,234 chars)
            2. read_document(doc_id="doc_abc123")
               â†’ Returns: Full 15,234 character document text
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "doc_id": {
                        "type": "string",
                        "description": "Document ID (e.g., 'doc_abc123def456') from search_onedrive results"
                    }
                },
                "required": ["doc_id"]
            }
        ),
        Tool(
            name="fuzzy_read",
            description="""
            ğŸ§  SMART READ - Find and open a document by description.

            This tool combines search and read into one step. It finds the MOST RELEVANT document
            matching your description and returns its full content immediately.

            WHEN TO USE:
            - When you are confident there is ONE specific document you want
            - When you want to "open" a file without searching first
            - E.g., "Read the Q4 marketing plan", "Open the resume for John Doe"

            HOW IT WORKS:
            1. Performs a full semantic search with your query
            2. Reranks results to find the absolute best match
            3. Returns the FULL content of the top result

            NOTE: If you are unsure if the document exists or want to see options,
            use 'search_onedrive' instead.
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Description of the file to open (e.g., 'Q3 financial report', 'meeting notes from last Friday')"
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="suggest_queries",
            description="""
            ğŸ’¡ GENERATE BETTER SEARCH QUERIES for improved results.

            Takes your original query and generates multiple optimized variations:
            - Removes filler words ("find", "show me", etc.)
            - Adds semantic context (e.g., "tax" â†’ "tax document IRS form")
            - Creates keyword-focused versions
            - Suggests alternative phrasings

            USE THIS WHEN:
            - Your initial search returns poor results
            - You want to explore different search angles
            - You're unsure how to phrase your query

            RETURNS:
            - List of 2-5 optimized query suggestions
            - Try each suggestion with search_onedrive to compare results
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Your original search query that needs improvement"
                    }
                },
                "required": ["query"]
            }
        )
    ]

@app.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    """Handle tool calls"""
    if name == "search_onedrive":
        original_query = arguments["query"]
        top_k = arguments.get("top_k", 5)

        reranked_results = await _perform_search(original_query, top_k)

        if not reranked_results:
            return [TextContent(
                type="text",
                text=f"âŒ No matching documents found for: '{original_query}'\n\n"
                     f"ğŸ’¡ Try:\n"
                     f"- Using different keywords\n"
                     f"- Being more specific\n"
                     f"- Using the 'suggest_queries' tool for better query ideas"
            )]

        # Format output
        output = f"ğŸ” Found {len(reranked_results)} results for: '{original_query}'\n"
        output += f"ğŸ§  Searched with AI reranking\n\n"

        # Format output with document summaries (prevent 1MB limit issues)
        MAX_PREVIEW_CHARS = 2000  # Show first 2000 chars per document
        MAX_TOTAL_SIZE = 900_000  # Keep total response under 900KB (well below 1MB limit)

        current_size = len(output)

        for i, (doc_id, doc_info) in enumerate(reranked_results, 1):
            # DEBUG: Log what we're returning in the response
            logging.info(f"  ğŸ“¤ Returning result {i} - doc_id: '{doc_id}', file: '{doc_info['file_name']}'")

            full_text = doc_info['full_text']
            text_length = len(full_text)

            # Truncate if needed
            if text_length > MAX_PREVIEW_CHARS:
                preview_text = full_text[:MAX_PREVIEW_CHARS] + f"\n\n... [Truncated: {text_length - MAX_PREVIEW_CHARS:,} more characters. Call read_document with doc_id={doc_id} for full text]"
            else:
                preview_text = full_text

            # Show blended score (more meaningful after reranking)
            score_display = f"{doc_info['blended_score']:.3f}"
            score_details = f"(Vector: {doc_info['original_score']:.3f}, Rerank: {doc_info['rerank_score']:.2f})"

            result_block = (
                f"**Result {i}** - Relevance Score: {score_display} {score_details}\n"
                f"ğŸ”‘ **DOC_ID: {doc_id}** (use this exact ID for read_document)\n"
                f"ğŸ“„ **File:** {doc_info['file_name']}\n"
                f"ğŸ“ **Path:** {doc_info['file_path']}\n"
                f"ğŸ“… **Modified:** {doc_info['modified']}\n"
                f"ğŸ“Š **Size:** {text_length:,} characters\n"
                f"ğŸ“ **Preview:**\n{preview_text}\n\n"
                f"---\n\n"
            )

            # Check if adding this result would exceed total size limit
            if current_size + len(result_block) > MAX_TOTAL_SIZE:
                output += f"\nâš ï¸ **Remaining results omitted** (response size limit reached)\n"
                break

            output += result_block
            current_size += len(result_block)

        return [TextContent(type="text", text=output)]

    elif name == "fuzzy_read":
        query = arguments["query"]
        logging.info(f"ğŸ§  Fuzzy read request: '{query}'")

        # Perform search looking for the single best match
        reranked_results = await _perform_search(query, top_k=1)

        if not reranked_results:
            return [TextContent(
                type="text",
                text=f"âŒ No matching documents found for: '{query}'"
            )]

        # Get the top result
        doc_id, doc_info = reranked_results[0]
        full_text = doc_info['full_text']

        logging.info(f"âœ… Fuzzy read found: {doc_info['file_name']} (Score: {doc_info['blended_score']:.3f})")

        output = (
            f"âœ… **Found Best Match:** {doc_info['file_name']}\n"
            f"ğŸ“ **Path:** {doc_info['file_path']}\n"
            f"ğŸ“Š **Relevance:** {doc_info['blended_score']:.3f}\n"
            f"ğŸ”‘ **ID:** {doc_id}\n\n"
            f"**Full Content:**\n\n"
            f"{full_text}"
        )

        return [TextContent(type="text", text=output)]

    elif name == "read_document":
        doc_id = arguments["doc_id"]

        logging.info(f"ğŸ“– Reading document: {doc_id}")

        # Retrieve full document text from Azure Blob Storage
        full_text = document_storage.retrieve_document(doc_id)

        if full_text is None:
            return [TextContent(
                type="text",
                text=f"âŒ Document not found: {doc_id}\n\nThe document may have been deleted or the doc_id may be incorrect."
            )]

        # Return full document with metadata
        output = (
            f"ğŸ“„ **Document ID:** {doc_id}\n"
            f"ğŸ“Š **Size:** {len(full_text):,} characters\n\n"
            f"**Full Text:**\n{full_text}"
        )

        logging.info(f"âœ… Retrieved document {doc_id} ({len(full_text):,} chars)")
        return [TextContent(type="text", text=output)]

    elif name == "suggest_queries":
        original_query = arguments["query"]

        logging.info(f"ğŸ’¡ Generating query suggestions for: '{original_query}'")

        # Generate enhanced query and variations
        enhanced = preprocess_query(original_query)
        variations = generate_query_variations(enhanced)

        # Format output
        output = f"ğŸ’¡ **Query Suggestions for:** '{original_query}'\n\n"
        output += "Here are optimized versions of your query that may yield better results:\n\n"

        for i, suggestion in enumerate(variations, 1):
            output += f"**{i}.** `{suggestion}`\n"
            if suggestion != original_query:
                # Explain what changed
                if len(suggestion.split()) < len(original_query.split()):
                    output += "   _(removed filler words for cleaner search)_\n"
                elif len(suggestion.split()) > len(original_query.split()):
                    output += "   _(added semantic context for better matching)_\n"
                else:
                    output += "   _(keyword-focused version)_\n"
            else:
                output += "   _(your original query)_\n"
            output += "\n"

        output += "\n**ğŸ’¡ Tips:**\n"
        output += "- Try each suggestion with `search_onedrive`\n"
        output += "- Combine keywords from multiple suggestions\n"
        output += "- Add specific dates, names, or file types if known\n"

        return [TextContent(type="text", text=output)]

    raise ValueError(f"Unknown tool: {name}")

if __name__ == "__main__":
    import asyncio
    import mcp.server.stdio
    
    async def main():
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await app.run(
                read_stream,
                write_stream,
                app.create_initialization_options()
            )
    
    asyncio.run(main())
